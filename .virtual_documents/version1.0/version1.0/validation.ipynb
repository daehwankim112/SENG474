import os
import json
import numpy as np
import pandas as pd
from collections import defaultdict
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt


df_features = pd.read_csv('scaled_songs.csv')
print(df_features.head)


np_features = df_features.to_numpy() 
print(np_features.shape)
np_features_copy = np_features[:,1:]
print(np_features_copy.shape)


import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np

from sklearn import datasets
from sklearn.preprocessing import MinMaxScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_predict

from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis 
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans


# Using Pca to decrease the dimension
pca = PCA(n_components=5)
X_pca = pca.fit_transform(np_features_copy)
print(X_pca[:5,:])


gmm = GaussianMixture(n_components=10, covariance_type='full', random_state=42)

gmm.fit(X_pca)

labels = gmm.predict(X_pca)

probs = gmm.predict_proba(X_pca)

print("Predict label for every data point:", labels)
print("The posterior probability of each data point is:\n", probs)


print(len(labels))


from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# Assume np_features_copy is your original normalized data, labels is the pseudo-labels obtained by clustering

# Divide the dataset: 80% training, 20% testing
X_train, X_test, y_train, y_test = train_test_split(np_features_copy, labels, test_size=0.2, random_state=42)

# Build and train a random forest model
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# Predict test set results
y_pred = clf.predict(X_test)

# Output evaluation results
print("Classification Report:")
print(classification_report(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))


from sklearn.svm import SVC

# Build and train the SVM model (the RBF kernel function is used here, and the kernel function and parameters can be adjusted according to the data)
svm_clf = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)
svm_clf.fit(X_train, y_train)

# Predict test set results
y_pred = svm_clf.predict(X_test)

# Output evaluation results
print("Classification Report:")
print(classification_report(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))


from sklearn.neighbors import NearestNeighbors
import numpy as np
from sklearn.preprocessing import StandardScaler

# pick one from dataset
new_data = np_features_copy[2,:]
print('input data:',new_data)
new_data = new_data.reshape(1, -1)

# using PCA to decrease dimension
new_data_pca = pca.transform(new_data)
predicted_label = gmm.predict(new_data_pca)[0]
print('predicted_label is:', predicted_label)

# select all data belong to this labels
cluster_indices = np.where(labels == predicted_label)[0]
cluster_data = X_pca[cluster_indices]


# using nearest neighbor search
nbrs = NearestNeighbors(n_neighbors=10, metric='euclidean')
nbrs.fit(cluster_data)
distances, local_indices = nbrs.kneighbors(new_data_pca)



recommended_indices = cluster_indices[local_indices[0]]
print("Recommended indices:", recommended_indices)
print("Corresponding distances:", distances)


import pandas as pd

# Assume X_pca is your dimensionally reduced data, with a shape of (n_samples, 5)
# Specify column names for each principal component (can be adjusted based on actual conditions)
columns = ["PC1", "PC2", "PC3", "PC4", "PC5"]

# create DataFrame
df_pca = pd.DataFrame(X_pca, columns=columns)

# Save as CSV file without saving row index
df_pca.to_csv("X_pca.csv", index=False)

print("X_pca already save as X_pca.csv")


columns = ["label"]

# create DataFrame
df_pca = pd.DataFrame(labels, columns=columns)

# Save as CSV file without saving row index
df_pca.to_csv("labels.csv", index=False)

print("labels was saved as labels.csv")



